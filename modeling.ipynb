{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258b37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67242d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfedb53350e4e808c9986d58585b453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad54355bb54744259cef4d613a42aa0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd327bda9204c7abc74700410c1ae08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0953dbc4bd8b4b2eaeba52ae934a4acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c121db10fc4748b305244259099ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9845e1e5a4b4438a9da2e48c507e923c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d23c536d104198af1a07db6b02ed6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fb155406154e1e801c5ff868be3e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9c7ac322dd46bb81205ec6c1da94e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8282f8eca24b6b82125a357ee4c6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2457a151d9b04dd698934831381d068e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a4c7c9743d41c38a1636b89bb76a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c014c24b66854fe696e8b9a57359b980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ccab3f542d40f69e8b4666d06c0472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb69a8c0525149c89e77c587642a6915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fd046886af4b458b0d60197ed56bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c9e06eeb9441f0ad3d0e3d4d1403f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4579a8e7273247d9b60c58e2e1a36f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84751f9498a4f6ab266ac8c86ffcf7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1bb877eb5c4d168169e58d67d9034b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6835c3291da54a098cb096dfcf33adb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5792076d476447d4868c21ae23d3eb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b934539e13104fc2aa3834babfd18939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77af902c9cbd4eb299298030c4b15176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0020a8091404dfabaa00fae50fc7cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/uzairpachhapure/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/uzairpachhapure/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question: what is lebesgue measure?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m tokenizers[i]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Encode all questions and user input\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m question_embeddings \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleanedQuestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m text: encode_text(text, model, tokenizer))\n\u001b[1;32m     55\u001b[0m user_input_embedding \u001b[38;5;241m=\u001b[39m encode_text(user_input, model, tokenizer)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Convert question embeddings to a 2D NumPy array\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 54\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     51\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m tokenizers[i]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Encode all questions and user input\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m question_embeddings \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleanedQuestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m text: encode_text(text, model, tokenizer))\n\u001b[1;32m     55\u001b[0m user_input_embedding \u001b[38;5;241m=\u001b[39m encode_text(user_input, model, tokenizer)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Convert question embeddings to a 2D NumPy array\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m, in \u001b[0;36mencode_text\u001b[0;34m(text, model, tokenizer)\u001b[0m\n\u001b[1;32m     32\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 34\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     35\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Average pooling over tokens\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1025\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1026\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1027\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1028\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1029\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1030\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1031\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    613\u001b[0m         hidden_states,\n\u001b[1;32m    614\u001b[0m         attention_mask,\n\u001b[1;32m    615\u001b[0m         layer_head_mask,\n\u001b[1;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    618\u001b[0m         past_key_value,\n\u001b[1;32m    619\u001b[0m         output_attentions,\n\u001b[1;32m    620\u001b[0m     )\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[1;32m    541\u001b[0m )\n\u001b[1;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pytorch_utils.py:239\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:464\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    465\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "# Load the pre-trained BERT models and tokenizers\n",
    "model_names = [\"bert-base-uncased\", \"bert-base-cased\", \"bert-large-uncased\", \"bert-large-cased\", \"bert-base-multilingual-cased\"]\n",
    "\n",
    "models = [AutoModel.from_pretrained(model_name) for model_name in model_names]\n",
    "tokenizers = [AutoTokenizer.from_pretrained(model_name) for model_name in model_names]\n",
    "\n",
    "data = pd.read_csv('/Users/uzairpachhapure/csu/Assignments/sem_4/project_final/new_data/cleaned_dat.csv')\n",
    "\n",
    "# Tokenize and clean the questions\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "data['CleanedQuestion'] = data['Question'].apply(preprocess_text)\n",
    "\n",
    "# Encode questions and user input using BERT\n",
    "def encode_text(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()  # Average pooling over tokens\n",
    "    return embeddings\n",
    "\n",
    "# Accept user input\n",
    "user_input = input(\"Enter your question: \")\n",
    "user_input = preprocess_text(user_input)\n",
    "\n",
    "# Initialize variables to keep track of the best model and its score\n",
    "best_models = []\n",
    "best_similarity_scores = []\n",
    "\n",
    "# Calculate cosine similarity scores for each model\n",
    "cosine_similarity_scores = []\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    model = models[i]\n",
    "    tokenizer = tokenizers[i]\n",
    "    \n",
    "    # Encode all questions and user input\n",
    "    question_embeddings = data['CleanedQuestion'].apply(lambda text: encode_text(text, model, tokenizer))\n",
    "    user_input_embedding = encode_text(user_input, model, tokenizer)\n",
    "    \n",
    "    # Convert question embeddings to a 2D NumPy array\n",
    "    question_embeddings = np.stack(question_embeddings)\n",
    "    \n",
    "    # Calculate cosine similarity between user input and dataset questions\n",
    "    similarity_scores = cosine_similarity([user_input_embedding], question_embeddings)\n",
    "    \n",
    "    # Store the cosine similarity scores for this model\n",
    "    cosine_similarity_scores.append(similarity_scores)\n",
    "    \n",
    "# Find the top 3 most similar questions for each model\n",
    "num_top_questions = 3\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    top_question_indices = np.argsort(cosine_similarity_scores[i])[0][-num_top_questions:][::-1]\n",
    "    top_similarity_scores = [cosine_similarity_scores[i][0][idx] for idx in top_question_indices]\n",
    "    \n",
    "    best_models.append(model_name)\n",
    "    best_similarity_scores.append(list(zip(data['Question'][top_question_indices], top_similarity_scores)))\n",
    "\n",
    "# Print the top 3 most similar questions and their cosine similarity scores for each model\n",
    "for i in range(len(best_models)):\n",
    "    print(f\"Model {i + 1}: {best_models[i]}\")\n",
    "    for j, (question, score) in enumerate(best_similarity_scores[i]):\n",
    "        print(f\"Top {j + 1} Question: {question}\")\n",
    "        print(f\"Cosine Similarity Score: {score:.4f}\")\n",
    "        print(\"-\" * 20)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae1f97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('/Users/uzairpachhapure/csu/Assignments/sem_4/project_final/new_data/cleaned_dat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d93bf85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600808, 12)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aeea5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = x.sample(n =10000, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "185c6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9a31690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/uzairpachhapure/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/uzairpachhapure/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Tokenize and clean the questions\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb89120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7262e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['CleanedQuestion'] = data['Question'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3951212e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question: hyperparameter\n",
      "Top 3 similar questions:\n",
      "Question 1: Kellogg' theorem: Let m\\ge1,0&lt;\\alpha&lt;1. If \\Omega\\in \\mathbb{R}^n is of class C^{m,\\alpha}, then \\log k\\in C^{m-1,\\alpha}(\\partial\\Omega), where k(y)=k_{x_0}(y)=k(x_0,y) is the Poisson kernel for the domain \\Omega and x_0\\in\\Omega is fixed.\n",
      "\n",
      "I am looking for the proof of this theorem, since I can not find it in Kellogg's book:Foundations of Potential Theory. I will be grateful if someone can tell me where I can get a complete proof for it.\n",
      "\n",
      "Answer 1: nan\n",
      "\n",
      "Question 2: Let X \\in [-n,n] \\cap \\mathbb Z be a sum of n iid Rademacher random variables and let X_1,\\ldots,X_k be k \\ge 1 iid copies of X, and define the product Z_k := X_1 X_2 \\ldots X_k. I'm interested in the moment-generating function (MGF) of Z, i.e \\eta_k(t) := \\mathbb E[e^{-tZ_k}]. Note that we can write X = 2B - n, where B is a Binomial r.v with parameter n and 1/2. Thus, by marginalizing w.r.t X_1, we have\n",
      "\n",
      "\\begin{split}\n",
      "\\eta_k(t) &amp;= \\mathbb E_{X_2,\\ldots,X_k} \\mathbb E_B[e^{-t(2B-n) Z_{k-1}}]= 2^{-n}\\sum_{j=0}^n {n \\choose j}\\mathbb E_{X_2,\\ldots,X_k} [e^{-t(2j-n)Z_{k-1}}]\\\\\n",
      "&amp;= 2^{-n}\\sum_{j=0}^n {n \\choose j}\\eta_{k-1}((2j-n)t).\n",
      "\\end{split}\n",
      "\n",
      "This gives a recursion formula for \\eta_k(t). The base case is \\eta_1(t) = \\mathbb E [e^{-tX_1}] = \\cosh^n(t).\n",
      "Question 1. What is a simple analytic formula for \\eta_k(t) (i.e solve the above recursion) ?\n",
      "N.B.: If I knew the generating z \\mapsto G(z,t):=\\sum_{k \\ge 0} \\eta_k(t) z^k function of the sequence (\\eta_k(t))_{k \\ge 0}, I could extract it like so: \\eta_k(t) = [z^k] G(z,t).\n",
      "Question 2. As a function of n and t \\gt 0, how fast does \\eta_k(t) grow ? If it helps, you may assume n \\ge C \\log k for some sufficiently large constant C. You may even assume \\log n \\asymp \\log k. Is \\eta_k(t) bounded in any of these regimes ?\n",
      "\n",
      "Answer 2: nan\n",
      "\n",
      "Question 3: Let (\\Omega,\\mathcal A,\\mu) be a \\sigma-finite measure space, \\mathbb P,\\mathbb Q be probability measures on (\\Omega,\\mathcal A) with density functions f,g: \\Omega \\to (0,\\infty) concerning \\mu.\n",
      "\n",
      "Prove that \\int_\\Omega\\min(f,g)\\text{d}\\mu\\ge\\frac{1}{2}\\left(\\int_\\Omega\\sqrt{f\\cdot g}\\ \\text{d}\\mu\\right)^2\n",
      "\n",
      "I have already tried everything I can think of, like with the Jensen-Inequality:\n",
      "\n",
      "\\frac{1}{2}\\left(\\int_\\Omega\\sqrt{f\\cdot g}\\ \\text{d}\\mu\\right)^2\\le\\int_\\Omega\\frac{f\\cdot g}{2}\\ \\text{d}\\mu but it all leads nowhere. Can anyone give me a hint? I greatly appreciate any help!\n",
      "\n",
      "Answer 3: This is just an application of Cauchy-Schwarz:\n",
      "\\begin{align} \n",
      "\\left( \\int_\\Omega \\sqrt[]{fg} \\, d\\mu\\right) ^2&amp;=\\left( \\int_\\Omega \\sqrt[]{\\min(f, g) \\max(f, g) } \\, d\\mu\\right) ^2 \\\\\n",
      "&amp;\\leq \\left( \\int_\\Omega \\min(f, g) \\, d\\mu\\right) \\left( \\int_\\Omega \\max(f, g) \\, d\\mu\\right)\n",
      "\\end{align} \n",
      "Now notice that \\max(f, g) \\leq f+g to finish. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Accept user input\n",
    "user_input = input(\"Enter your question: \")\n",
    "user_input = preprocess_text(user_input)\n",
    "\n",
    "# Vectorize the questions and user input\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['CleanedQuestion'])\n",
    "user_input_vector = tfidf_vectorizer.transform([user_input])\n",
    "\n",
    "# Calculate cosine similarity between user input and dataset questions\n",
    "similarity_scores = cosine_similarity(user_input_vector, tfidf_matrix)\n",
    "\n",
    "# Find the indices of the top 3 most similar questions\n",
    "top_similar_indices = similarity_scores.argsort()[0][-3:][::-1]\n",
    "\n",
    "# Retrieve the corresponding answers for the top 3 questions\n",
    "top_similar_questions = [data.iloc[index]['Question'] for index in top_similar_indices]\n",
    "top_similar_answers = [data.iloc[index]['Answer'] for index in top_similar_indices]\n",
    "\n",
    "# Print the top 3 similar questions and corresponding answers\n",
    "print(\"Top 3 similar questions:\")\n",
    "for i, question in enumerate(top_similar_questions):\n",
    "    print(f\"Question {i + 1}: {question}\")\n",
    "    print(f\"Answer {i + 1}: {top_similar_answers[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "56eebffa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===-----------------------------------------------] 6.2% 23.4/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======--------------------------------------------] 12.6% 47.4/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========------------------------------------------] 16.2% 61.0/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============--------------------------------------] 24.3% 91.4/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================---------------------------------] 35.1% 132.1/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================----------------------------] 45.6% 171.6/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================----------------------] 57.2% 215.3/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================----------------] 68.5% 257.8/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================================-----------] 78.8% 296.3/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================------] 88.7% 333.5/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 99.0% 372.5/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question: I'm struggling to grasp intuition behind arc function. I know that it is a inverse function to corresponding trigonometric function (\\sin,\\cos...). Why their graph looks like it does?\n",
      "\n",
      "\n",
      "Similar Question: 1  :  I'm struggling to grasp intuition behind arc function. I know that it is a inverse function to corresponding trigonometric function (\\sin,\\cos...).\n",
      "Why their graph looks like it does?\n",
      "\n",
      "Answer: There is a general answer for that (see the picture below): if you know the graph of f, then the graph of f^{-1} is that of f reflected about the line y=x.\n",
      "\n",
      "\n",
      "Cosine Similarity: 0.9999999\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Similar Question: 2  :  I need to prove that the inverse function of a linear function is also linear? I suppose that a linear function is defined like this: f(x)=ax+b but how can I prove that its inverse is linear?\n",
      "\n",
      "Answer: y=ax+b\\implies x=\\frac{y-b}a=\\frac1ay-\\frac ba\n",
      "\n",
      "In the comments it was already explained why it has to be \\;a\\neq 0\\; ...\n",
      "\n",
      "Cosine Similarity: 0.94047093\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Similar Question: 3  :  I have a function f(x), using the logic it's relatively easy to formally define what the inverse should be like, relatively to domain and codomain elements, especially using the surjective and injective definitions; my problem is how I can algorithmically find the inverse of a function given the definition f(x)=y where y is the set of algebraic steps I take to compute f(x) ? \n",
      "\n",
      "Answer: nan\n",
      "Cosine Similarity: 0.9370562\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df = pd.read_csv('/Users/uzairpachhapure/csu/Assignments/sem_4/project_final/new_data/cleaned_dat.csv')\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "questions = df[\"Question\"].tolist()\n",
    "answers = df[\"Answer\"].tolist()\n",
    "\n",
    "# Load the pre-trained GloVe model (replace 'glove-wiki-gigaword-300' with the desired model name)\n",
    "glove_model = api.load('glove-wiki-gigaword-300')\n",
    "\n",
    "# Function to calculate the average word vector for a sentence\n",
    "def average_word_vectors(sentence, model, num_features):\n",
    "    words = sentence.split()\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
    "    num_words = 0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            num_words += 1\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    if num_words > 0:\n",
    "        feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector\n",
    "\n",
    "# Create word vectors for questions\n",
    "question_vectors = [average_word_vectors(question, glove_model, num_features=300) for question in questions]\n",
    "\n",
    "# Get user input\n",
    "user_input = input(\"Enter your question: \")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Preprocess user input and transform into a word vector\n",
    "user_vector = average_word_vectors(user_input, glove_model, num_features=300)\n",
    "\n",
    "# Calculate cosine similarities between user input and dataset questions\n",
    "similarities = [cosine_similarity([user_vector], [question_vector])[0][0] for question_vector in question_vectors]\n",
    "\n",
    "# Get the indices of the top 3 most similar questions\n",
    "top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:3]\n",
    "\n",
    "i = 1\n",
    "# Retrieve and print corresponding answers\n",
    "for idx in top_indices:\n",
    "    print(\"Similar Question:\", i, \" : \", questions[idx])\n",
    "    i = i + 1\n",
    "    print(\"Answer:\", answers[idx])\n",
    "    print(\"Cosine Similarity:\", similarities[idx])\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b509cba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: pandas in /Users/uzairpachhapure/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: gensim in /Users/uzairpachhapure/anaconda3/lib/python3.11/site-packages (4.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/uzairpachhapure/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/uzairpachhapure/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/uzairpachhapure/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/uzairpachhapure/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/uzairpachhapure/anaconda3/lib/python3.11/site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/uzairpachhapure/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim)\n",
      "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/uzairpachhapure/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting simpful (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading simpful-2.11.0-py3-none-any.whl (32 kB)\n",
      "Collecting fst-pso (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting miniful (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20430 sha256=549dad208012f4bb954f02f983a8011def1b13016090f8d2bf5fcdfe6d8a24de\n",
      "  Stored in directory: /Users/uzairpachhapure/Library/Caches/pip/wheels/69/f5/e5/18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3513 sha256=1da39c046e24eeb741f2cd66c990c605de75318e159460f60c22924e9789ac4b\n",
      "  Stored in directory: /Users/uzairpachhapure/Library/Caches/pip/wheels/9d/ff/2f/afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: simpful, miniful, fst-pso, pyfume, FuzzyTM\n",
      "Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 miniful-0.0.6 pyfume-0.2.25 simpful-2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "419bb67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your math-related question: formula to calculate mean\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m user_vector \u001b[38;5;241m=\u001b[39m average_word_vectors(user_input, glove_model, num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarities between user input and dataset questions\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m similarities \u001b[38;5;241m=\u001b[39m [cosine_similarity([user_vector], [question_vector])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m question_vector \u001b[38;5;129;01min\u001b[39;00m question_vectors]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Get the indices of the top 3 most similar questions\u001b[39;00m\n\u001b[1;32m     38\u001b[0m top_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(similarities)), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m i: similarities[i], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:\u001b[38;5;241m3\u001b[39m]\n",
      "Cell \u001b[0;32mIn[66], line 35\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m user_vector \u001b[38;5;241m=\u001b[39m average_word_vectors(user_input, glove_model, num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarities between user input and dataset questions\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m similarities \u001b[38;5;241m=\u001b[39m [cosine_similarity([user_vector], [question_vector])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m question_vector \u001b[38;5;129;01min\u001b[39;00m question_vectors]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Get the indices of the top 3 most similar questions\u001b[39;00m\n\u001b[1;32m     38\u001b[0m top_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(similarities)), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m i: similarities[i], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1583\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1581\u001b[0m     Y_normalized \u001b[38;5;241m=\u001b[39m X_normalized\n\u001b[1;32m   1582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1583\u001b[0m     Y_normalized \u001b[38;5;241m=\u001b[39m normalize(Y, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1585\u001b[0m K \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X_normalized, Y_normalized\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39mdense_output)\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    186\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:1843\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# axis == 1:\u001b[39;00m\n\u001b[1;32m   1841\u001b[0m     sparse_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1843\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1844\u001b[0m     X,\n\u001b[1;32m   1845\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39msparse_format,\n\u001b[1;32m   1846\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   1847\u001b[0m     estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe normalize function\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1848\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[1;32m   1849\u001b[0m )\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1851\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    883\u001b[0m     array \u001b[38;5;241m=\u001b[39m _ensure_sparse_format(\n\u001b[1;32m    884\u001b[0m         array,\n\u001b[1;32m    885\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    892\u001b[0m     )\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;66;03m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;66;03m# to an error. This is needed because specifying a non complex\u001b[39;00m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;66;03m# dtype to the function converts complex to real dtype,\u001b[39;00m\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;66;03m# thereby passing the test made in the lines following the scope\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;66;03m# of warnings context manager.\u001b[39;00m\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m             warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, ComplexWarning)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "questions = df[\"Question\"].tolist()\n",
    "answers = df[\"Answer\"].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate the average word vector for a sentence\n",
    "def average_word_vectors(sentence, model, num_features):\n",
    "    words = sentence.split()\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
    "    num_words = 0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            num_words += 1\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    if num_words > 0:\n",
    "        feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector\n",
    "\n",
    "# Create word vectors for questions\n",
    "question_vectors = [average_word_vectors(question, glove_model, num_features=300) for question in questions]\n",
    "\n",
    "# Get user input\n",
    "user_input = input(\"Enter your math-related question: \")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Preprocess user input and transform into a word vector\n",
    "user_vector = average_word_vectors(user_input, glove_model, num_features=300)\n",
    "\n",
    "# Calculate cosine similarities between user input and dataset questions\n",
    "similarities = [cosine_similarity([user_vector], [question_vector])[0][0] for question_vector in question_vectors]\n",
    "\n",
    "# Get the indices of the top 3 most similar questions\n",
    "top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:3]\n",
    "\n",
    "i = 1\n",
    "# Retrieve and print corresponding answers\n",
    "for idx in top_indices:\n",
    "    print(\"Similar Question:\", i, \" : \", questions[idx])\n",
    "    i = i + 1\n",
    "    print(\"Answer:\", answers[idx])\n",
    "    print(\"Cosine Similarity:\", similarities[idx])\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b80cb636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your math-related question (type 'exit' to end): formula to calculate mean\n",
      "\n",
      "Processing...\n",
      "Similar Question: 1  :  Why \\sum_{k=1}^n \\left(\\frac56\\right)^{k-1}\\cdot \\left(\\frac16\\right)=1-\\left(\\frac56\\right)^n??\n",
      "Which formula I need to use to calculate it?\n",
      "\n",
      "Thank you!\n",
      "\n",
      "Answer: \\sum_{k=1}^n\\left(\\frac56\\right)^{k-1}\\frac16=\\frac15\\sum_{k=1}^n\\left(\\frac56\\right)^k\\stackrel{\\text{geom. series}}=\\frac15\\frac{\\frac56-\\left(\\frac56\\right)^{n+1}}{1-\\frac56}=\n",
      "\n",
      "{}\n",
      "\n",
      "=\\frac65\\left[\\frac56-\\left(\\frac56\\right)^{n+1}\\right]=1-\\left(\\frac56\\right)^n\n",
      "\n",
      "Cosine Similarity: 0.9008409\n",
      "\n",
      "\n",
      "Similar Question: 2  :  Show that following formula is true:\n",
      "\n",
      "\n",
      "\\sum_{i=0}^{[n/2]}(-1)^i (n-2i)^n{n \\choose i}=2^{n-1}n!\n",
      "\n",
      "\n",
      "Using formula calculate\n",
      "\n",
      "\\sum_{i=0}^n(2i-n)^p{p \\choose i}\n",
      "\n",
      "\n",
      "Answer: nan\n",
      "Cosine Similarity: 0.867305\n",
      "\n",
      "\n",
      "Similar Question: 3  :  f(t)=\\frac{\\sin^3(\\pi t)}{(\\pi t)^3}\n",
      "I want to calculate the Fourier transform.\n",
      "I can't calculate this integral:\n",
      "\\int_0^\\infty\\frac{\\sin^3(\\pi t)}{(\\pi t)^3}\\cos(ut)\\,\\mathrm{d}t\n",
      "\n",
      "Answer: nan\n",
      "Cosine Similarity: 0.859267\n",
      "\n",
      "\n",
      "Processing time: 151.30 seconds\n",
      "\n",
      "Enter your math-related question (type 'exit' to end): exit\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "questions = df[\"Question\"].tolist()\n",
    "answers = df[\"Answer\"].tolist()\n",
    "\n",
    "#\n",
    "\n",
    "# Function to calculate the average word vector for a sentence\n",
    "def average_word_vectors(sentence, model, num_features):\n",
    "    words = sentence.split()\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
    "    num_words = 0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            num_words += 1\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    if num_words > 0:\n",
    "        feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector\n",
    "\n",
    "# Create word vectors for questions\n",
    "question_vectors = [average_word_vectors(question, glove_model, num_features=300) for question in questions]\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"Enter your math-related question (type 'exit' to end): \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    print(\"\\nProcessing...\")\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Preprocess user input and transform into a word vector\n",
    "    user_vector = average_word_vectors(user_input, glove_model, num_features=300)\n",
    "\n",
    "    # Calculate cosine similarities between user input and dataset questions\n",
    "    similarities = [cosine_similarity([user_vector], [question_vector])[0][0] for question_vector in question_vectors]\n",
    "\n",
    "    # Get the indices of the top 3 most similar questions\n",
    "    top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:3]\n",
    "\n",
    "    # Retrieve and print corresponding answers\n",
    "    i = 1\n",
    "    for idx in top_indices:\n",
    "        print(\"Similar Question:\", i, \" : \", questions[idx])\n",
    "        i = i + 1\n",
    "        print(\"Answer:\", answers[idx])\n",
    "        print(\"Cosine Similarity:\", similarities[idx])\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Calculate and print the elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Processing time: {elapsed_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "06b77a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your math-related question (type 'exit' to end): formula to calculate mean\n",
      "\n",
      "Processing...\n",
      "Similar Question: 1  :  Why \\sum_{k=1}^n \\left(\\frac56\\right)^{k-1}\\cdot \\left(\\frac16\\right)=1-\\left(\\frac56\\right)^n??\n",
      "Which formula I need to use to calculate it?\n",
      "\n",
      "Thank you!\n",
      "\n",
      "Answer: \\sum_{k=1}^n\\left(\\frac56\\right)^{k-1}\\frac16=\\frac15\\sum_{k=1}^n\\left(\\frac56\\right)^k\\stackrel{\\text{geom. series}}=\\frac15\\frac{\\frac56-\\left(\\frac56\\right)^{n+1}}{1-\\frac56}=\n",
      "\n",
      "{}\n",
      "\n",
      "=\\frac65\\left[\\frac56-\\left(\\frac56\\right)^{n+1}\\right]=1-\\left(\\frac56\\right)^n\n",
      "\n",
      "Cosine Similarity: 0.867305\n",
      "\n",
      "\n",
      "Similar Question: 2  :  Show that following formula is true:\n",
      "\n",
      "\n",
      "\\sum_{i=0}^{[n/2]}(-1)^i (n-2i)^n{n \\choose i}=2^{n-1}n!\n",
      "\n",
      "\n",
      "Using formula calculate\n",
      "\n",
      "\\sum_{i=0}^n(2i-n)^p{p \\choose i}\n",
      "\n",
      "\n",
      "Answer: nan\n",
      "Cosine Similarity: 0.859267\n",
      "\n",
      "\n",
      "Similar Question: 3  :  f(t)=\\frac{\\sin^3(\\pi t)}{(\\pi t)^3}\n",
      "I want to calculate the Fourier transform.\n",
      "I can't calculate this integral:\n",
      "\\int_0^\\infty\\frac{\\sin^3(\\pi t)}{(\\pi t)^3}\\cos(ut)\\,\\mathrm{d}t\n",
      "\n",
      "Answer: nan\n",
      "Cosine Similarity: 0.85915977\n",
      "\n",
      "\n",
      "Processing time: 120.67 seconds\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m question_vectors \u001b[38;5;241m=\u001b[39m [average_word_vectors(question, glove_model, num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Get user input\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter your math-related question (type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to end): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1206\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1207\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "questions = df[\"Question\"].tolist()\n",
    "answers = df[\"Answer\"].tolist()\n",
    "\n",
    "# Create an inverted index for the \"questions\" column\n",
    "inverted_index = {}\n",
    "for idx, question in enumerate(questions):\n",
    "    words = question.split()\n",
    "    for word in words:\n",
    "        word = word.lower()  # Convert to lowercase to make the search case-insensitive\n",
    "        if word not in inverted_index:\n",
    "            inverted_index[word] = []\n",
    "        inverted_index[word].append(idx)\n",
    "\n",
    "# Function to calculate the average word vector for a sentence\n",
    "def average_word_vectors(sentence, model, num_features):\n",
    "    words = sentence.split()\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
    "    num_words = 0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            num_words += 1\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    if num_words > 0:\n",
    "        feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector\n",
    "\n",
    "# Create word vectors for questions\n",
    "question_vectors = [average_word_vectors(question, glove_model, num_features=300) for question in questions]\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"Enter your math-related question (type 'exit' to end): \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    print(\"\\nProcessing...\")\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Preprocess user input and transform into a word vector\n",
    "    user_vector = average_word_vectors(user_input, glove_model, num_features=300)\n",
    "\n",
    "    # Calculate cosine similarities between user input and dataset questions\n",
    "    similarities = []\n",
    "\n",
    "    # Search for matching words in the inverted index\n",
    "    words = user_input.lower().split()\n",
    "    matching_question_indices = set()\n",
    "    for word in words:\n",
    "        if word in inverted_index:\n",
    "            matching_question_indices.update(inverted_index[word])\n",
    "\n",
    "    # Calculate similarities only for the matching questions\n",
    "    for idx in matching_question_indices:\n",
    "        question_vector = question_vectors[idx]\n",
    "        similarity = cosine_similarity([user_vector], [question_vector])[0][0]\n",
    "        similarities.append((idx, similarity))\n",
    "\n",
    "    # Sort by similarity score and get the top 3 most similar questions\n",
    "    sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    top_indices = [idx for idx, _ in sorted_similarities[:3]]\n",
    "\n",
    "    # Retrieve and print corresponding answers\n",
    "    i = 1\n",
    "    for idx in top_indices:\n",
    "        print(\"Similar Question:\", i, \" : \", questions[idx])\n",
    "        i = i + 1\n",
    "        print(\"Answer:\", answers[idx])\n",
    "        print(\"Cosine Similarity:\", sorted_similarities[i - 1][1])\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Calculate and print the elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Processing time: {elapsed_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b43c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a3d52c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example . How to remove it? Another random math expression ...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a2243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
